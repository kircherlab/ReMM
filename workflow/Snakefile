"""
ReMM score workflow

This workflow is for generating the ReMM score is consists on multiple sub workflows. For generating the WG score file the main modules are:
    - Feature generation (features.smk)
    - Variant processing (variants.smk)
    - Annotation of variants (annotate.smk)
    - Training of variants (training.smk)
    - Whole genome scores (scores.smk)

There are additional workflows:
    - Evaluation of classifiers or predictions (evaluation.smk)


The workflow is completly configured via the config file (config/config.yaml).
"""

#### imports ####
from snakemake.utils import validate
from os.path import join
from glob import glob
import datetime
import pandas as pd


##### check snakemake min version #####

from snakemake.utils import min_version

min_version("6.10.0")


##### load config and sample sheets #####


configfile: "config/config.yaml"


validate(config, schema="schemas/config.schema.yaml")


# FIXME: Why this is necessary? Maybe can be removed!
configfile: "config/featuresConfig38.json"


def getWrapper(wrapper):
    """
    Get directory for snakemake wrappers.
    """
    return "file:%s/%s/wrapper.py" % (
        config["global_files"]["wrapper_directory"],
        wrapper,
    )


##### include workflow files #####


# Workflow features
include: "rules/features.smk"
# Workflow variant generation
include: "rules/variant_generation.smk"
# Workflow variants
include: "rules/variants.smk"
# Workflow annotate
include: "rules/annotate.smk"
# Workflow training
include: "rules/training.smk"
# Workflow evaluation
include: "rules/evaluation.smk"
# Workflow predictions
include: "rules/predictions.smk"
# Workflow feature_importance
include: "rules/feature_importance.smk"
# Workflow scores
include: "rules/scores.smk"
# Workflow correlation
include: "rules/correlation.smk"


# Workflow predict

# Workflow replace

# old rules


include: "rules/process/getUtils.smk"
include: "rules/process/createModelData.smk"


###################################
### All rules to generate files ###
###################################


def getAllVariantsInput():
    """
    Getter for all variants.
    """
    output = []
    for variant_set in config["variants"]:
        output += getVariantsInput(variant_set, "annotate")
    return output


# ALL rule. Generates variants and feature sets.
# TODO Add all configs to all rule
rule all:
    input:
        # variants
        getAllVariantsInput(),
        # feature sets
        expand(
            "results/evaluation/{training_run}/metrics/auc.tsv.gz",
            training_run=list(config["training"].keys()),
        ),
        expand(
            "results/evaluation/{training_run}/metrics/curves_prc_roc.png",
            training_run=list(config["training"].keys()),
        ),
        vcf=expand(
            "results/features/feature_sets/{feature_set}.vcf.gz",
            feature_set=list(config["feature_sets"].keys()),
        ),
        idx=expand(
            "results/features/feature_sets/{feature_set}.vcf.gz.tbi",
            feature_set=list(config["feature_sets"].keys()),
        ),


#######################################
### All rules for workflow features ###
#######################################
def getAllAverageFilesForFeatures(genomeBuild):
    """
    Get a list of features for a genome build that are defined in the feature config file as well as in the config file.
    Use them to to return the average feature path.
    """
    featureList = getDefinedFeatures(genomeBuild)
    return expand(
        "results/features/single_vcf/{feature}/{genomeBuild}/single/{feature}.avg.tsv.gz",
        feature=featureList,
        genomeBuild=genomeBuild,
    )


rule all_feature_average:
    """
    Get feature averages. Needed to add in the config file before starting annotation.
    """
    input:
        getAllAverageFilesForFeatures("hg19"),
        getAllAverageFilesForFeatures("hg38"),


########################################
# All rule for generating feature sets #
########################################
rule all_feature_sets:
    input:
        vcf=expand(
            "results/features/feature_sets/{feature_set}.vcf.gz",
            feature_set=list(config["feature_sets"].keys()),
        ),
        idx=expand(
            "results/features/feature_sets/{feature_set}.vcf.gz.tbi",
            feature_set=list(config["feature_sets"].keys()),
        ),


###################################
# All rules for workflow variants #
###################################


# all rule to return all variant files
rule all_variants:
    input:
        getAllVariantsInput(),


###################################
# All rules for workflow annotate #
###################################
def getAllAnnotations():
    """
    Get all positive and negatuve training annotations
    TODO: Update for other annotations like benchmark sets
    """
    output = []
    for training in config["training"].values():
        output += [
            "results/annotation/%s/%s.%s.sorted.tsv.gz"
            % (
                training["positives"],
                training["positives"],
                training["feature_set"],
            )
        ]
        output += [
            "results/annotation/%s/%s.%s.sorted.tsv.gz"
            % (
                training["negatives"],
                training["negatives"],
                training["feature_set"],
            )
        ]
    return output


# all rule to return all annotated variants
rule all_annotation:
    input:
        getAllAnnotations(),


######################################
### All rules for workflow evaluate ##
######################################
rule all_evaluate:
    input:
        expand(
            "results/evaluation/{training_run}/metrics/auc.tsv.gz",
            training_run=list(config["training"].keys()),
        ),
        expand(
            "results/evaluation/{training_run}/metrics/curves_prc_roc.png",
            training_run=list(config["training"].keys()),
        ),
        expand(
            "results/evaluation/{training_run}/metrics/repetitive/auc_all.mean_max_min.tsv.gz",
            training_run=list(config["training"].keys()),
        ),


#################################################
### All rules for workflow feature importance ###
#################################################
rule all_featureImportance:
    input:
        expand(
            "results/training/{training_run}/feature_importance/feature_importance.names.mean_max_min.tsv.gz",
            training_run=list(config["training"].keys()),
        ),


####################################
### All rules for workflow score ###
####################################
rule all_scores:
    input:
        expand(
            "results/scores/{score_name}/release/{score_name}.unbiased.tsv.gz{idx}",
            score_name=list(config["scores"].keys()),
            idx=["", ".tbi"],
        ),


##########################################
### All rules for workflow correlation ###
##########################################
def getCorrelationPlots():
    output = []
    for correlation, values in config["correlation"].items():
        if "plot" in values["correlate"]:
            output += expand(
                "results/correlation/{correlation}/{input_type}.correlate.{method}.png",
                correlation=correlation,
                input_type=values["correlate"]["plot"],
                method=["spearman", "pearson"],
            )
    return output


rule all_correlation:
    input:
        expand(
            "results/correlation/{correlation}/{input_type}.correlate.tsv.gz",
            correlation=list(config["correlation"].keys()),
            input_type=["feature", "score"],
        ),
        getCorrelationPlots(),


rule all_correlation_plots:
    input:
        getCorrelationPlots(),
